{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd601d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy matplotlib seaborn nltk tqdm librosa soundfile praat-parselmouth opensmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467269a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16622c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_integrated_stance_vector(row):\n",
    "    # ------ DIMENSIONAL COMPONENTS ------\n",
    "    \n",
    "    # VALENCE (positive-negative) - blend acoustic and lexical features\n",
    "    valence = (\n",
    "        0.3 * standardize(row['std_pitch']) +             # Pitch variability\n",
    "        0.2 * standardize(row['speech_rate']) +           # Speech rate\n",
    "        0.3 * row['posemo'] -                             # Positive emotion words\n",
    "        0.2 * row['negemo']                               # Negative emotion words\n",
    "    )\n",
    "    \n",
    "    # AROUSAL (activation) - blend acoustic and lexical features\n",
    "    arousal = (\n",
    "        0.3 * standardize(row['mean_energy']) +           # Energy level\n",
    "        0.2 * standardize(row['speech_rate']) +           # Speech rate\n",
    "        0.2 * standardize(row['std_energy']) +            # Energy variability\n",
    "        0.1 * (row['posemo'] + row['negemo']) +           # Emotional intensity\n",
    "        0.1 * row['interrog'] +                           # Questions indicate engagement\n",
    "        0.1 * row['has_exclamation']                      # Exclamations indicate high arousal\n",
    "    )\n",
    "    \n",
    "    # DOMINANCE (power/control) - blend acoustic and lexical features\n",
    "    dominance = (\n",
    "        0.2 * standardize(row['mean_energy']) +           # Energy level\n",
    "        0.2 * row['certain'] -                            # Certainty words\n",
    "        0.2 * row['tentat'] +                             # Absence of tentative language\n",
    "        0.2 * row['power'] +                              # Power-related words\n",
    "        0.1 * (1 - row['has_question_mark']) +            # Absence of questions\n",
    "        0.1 * row['i']                                    # Self-reference (\"I\" statements)\n",
    "    )\n",
    "    \n",
    "    # ------ CATEGORICAL EMOTION COMPONENTS ------\n",
    "    \n",
    "    # Initialize emotion scores\n",
    "    emotions = {\n",
    "        'happiness': 0.1,\n",
    "        'sadness': 0.1,\n",
    "        'anger': 0.1,\n",
    "        'fear': 0.1,\n",
    "        'disgust': 0.1,\n",
    "        'surprise': 0.1\n",
    "    }\n",
    "    \n",
    "    # HAPPINESS - combine acoustic and lexical cues\n",
    "    happiness_score = (\n",
    "        0.3 * standardize(max(0, row['mean_pitch'])) +     # Higher pitch\n",
    "        0.2 * standardize(max(0, row['std_pitch'])) +      # Pitch variability\n",
    "        0.2 * standardize(max(0, row['speech_rate'])) +    # Faster speech\n",
    "        0.3 * row['posemo']                                # Positive emotion words\n",
    "    )\n",
    "    emotions['happiness'] += max(0, happiness_score)\n",
    "    \n",
    "    # SADNESS - combine acoustic and lexical cues\n",
    "    sadness_score = (\n",
    "        0.3 * standardize(min(0, -row['mean_pitch'])) +    # Lower pitch\n",
    "        0.2 * standardize(min(0, -row['std_pitch'])) +     # Less pitch variability\n",
    "        0.2 * standardize(min(0, -row['speech_rate'])) +   # Slower speech\n",
    "        0.3 * row['sad']                                   # Sadness-related words\n",
    "    )\n",
    "    emotions['sadness'] += max(0, sadness_score)\n",
    "    \n",
    "    # ANGER - combine acoustic and lexical cues\n",
    "    anger_score = (\n",
    "        0.3 * standardize(max(0, row['mean_energy'])) +    # Higher energy\n",
    "        0.2 * standardize(max(0, row['std_energy'])) +     # Energy variability\n",
    "        0.2 * row['anger'] +                               # Anger-related words\n",
    "        0.2 * row['swear'] +                               # Swear words\n",
    "        0.1 * row['negate']                                # Negations\n",
    "    )\n",
    "    emotions['anger'] += max(0, anger_score)\n",
    "    \n",
    "    # FEAR - combine acoustic and lexical cues\n",
    "    fear_score = (\n",
    "        0.3 * standardize(max(0, row['std_pitch'])) +      # Pitch variability\n",
    "        0.2 * standardize(max(0, row['speech_rate'])) +    # Faster speech\n",
    "        0.3 * row['anx'] +                                 # Anxiety-related words\n",
    "        0.1 * row['tentat'] +                              # Tentative language\n",
    "        0.1 * row['risk']                                  # Risk-related words\n",
    "    )\n",
    "    emotions['fear'] += max(0, fear_score)\n",
    "    \n",
    "    # DISGUST - harder to detect, but using available cues\n",
    "    disgust_score = (\n",
    "        0.4 * row['negemo'] +                              # Negative emotion words\n",
    "        0.3 * standardize(min(0, -row['mean_pitch'])) +    # Lower pitch\n",
    "        0.3 * row['negate']                                # Negations\n",
    "    )\n",
    "    emotions['disgust'] += max(0, disgust_score)\n",
    "    \n",
    "    # SURPRISE - combine acoustic and lexical cues\n",
    "    surprise_score = (\n",
    "        0.3 * standardize(max(0, row['max_pitch'])) +      # Pitch peaks\n",
    "        0.3 * standardize(max(0, row['std_energy'])) +     # Energy variability\n",
    "        0.2 * row['interrog'] +                            # Questions\n",
    "        0.2 * row['has_exclamation']                       # Exclamations\n",
    "    )\n",
    "    emotions['surprise'] += max(0, surprise_score)\n",
    "    \n",
    "    # Normalize emotion probabilities\n",
    "    total = sum(emotions.values())\n",
    "    emotions = {k: v/total for k, v in emotions.items()}\n",
    "    \n",
    "    # ------ DIALOG ACT SPECIFIC COMPONENTS ------\n",
    "    \n",
    "    # CERTAINTY - relevant for statements, opinions, agreements\n",
    "    certainty = (\n",
    "        0.3 * row['certain'] -\n",
    "        0.3 * row['tentat'] -\n",
    "        0.2 * row['interrog'] +\n",
    "        0.2 * (1 - row['has_question_mark'])\n",
    "    )\n",
    "    \n",
    "    # AGREEMENT - relevant for agreement, acceptance, understanding\n",
    "    agreement = (\n",
    "        0.4 * row['assent'] +\n",
    "        0.3 * row['has_affirmation_word'] -\n",
    "        0.3 * row['has_negation_word']\n",
    "    )\n",
    "    \n",
    "    # ENGAGEMENT - relevant for backchannels, questions\n",
    "    engagement = (\n",
    "        0.3 * row['interrog'] +\n",
    "        0.2 * row['has_question_word'] +\n",
    "        0.2 * row['has_you_know'] +\n",
    "        0.3 * row['has_do_you']\n",
    "    )\n",
    "    \n",
    "    # Construct the final stance vector - map all to 0-1 scale\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Return the complete stance vector\n",
    "    return {\n",
    "        # Dimensional components\n",
    "        'valence': sigmoid(valence),\n",
    "        'arousal': sigmoid(arousal),\n",
    "        'dominance': sigmoid(dominance),\n",
    "        \n",
    "        # Categorical emotion components\n",
    "        'happiness': emotions['happiness'],\n",
    "        'sadness': emotions['sadness'],\n",
    "        'anger': emotions['anger'],\n",
    "        'fear': emotions['fear'],\n",
    "        'disgust': emotions['disgust'],\n",
    "        'surprise': emotions['surprise'],\n",
    "        \n",
    "        # Dialog act specific components\n",
    "        'certainty': sigmoid(certainty),\n",
    "        'agreement': sigmoid(agreement),\n",
    "        'engagement': sigmoid(engagement)\n",
    "    }\n",
    "\n",
    "# Helper function to standardize values (assumes they are already z-scored)\n",
    "def standardize(value):\n",
    "    return max(-3, min(3, value)) / 3  # Clip to [-3, 3] and scale to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a49f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load both datasets\n",
    "speech_df = pd.read_csv('speech_features_train_with_speech_rate.csv')\n",
    "text_df = pd.read_csv('text_features_train.csv')\n",
    "\n",
    "# Merge on dialog_id, speaker, and time information\n",
    "merged_df = pd.merge(\n",
    "    speech_df, \n",
    "    text_df,\n",
    "    on=['dialog_id', 'speaker', 'da_tag', 'start_time', 'end_time'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Apply the function to create stance vectors\n",
    "df_stance = merged_df.apply(create_integrated_stance_vector, axis=1, result_type='expand')\n",
    "\n",
    "# Add dialog act labels\n",
    "df_stance['da_tag'] = merged_df['da_tag']\n",
    "\n",
    "# Save the stance vectors to a CSV file\n",
    "df_stance.to_csv('stance_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d78d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "Accuracy: 0.7250\n",
      "Macro F1 Score: 0.4160\n",
      "\n",
      "Detailed Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           %       0.32      0.24      0.28       615\n",
      "          aa       0.44      0.27      0.33       470\n",
      "           b       0.73      0.83      0.78      1489\n",
      "          ba       0.43      0.33      0.37       160\n",
      "          fc       0.13      0.07      0.10        95\n",
      "          ny       0.13      0.03      0.06       115\n",
      "          qy       0.54      0.26      0.35       188\n",
      "          sd       0.64      0.81      0.71      2873\n",
      "          sv       0.34      0.17      0.23      1058\n",
      "           x       0.93      0.97      0.95      4054\n",
      "\n",
      "    accuracy                           0.73     11117\n",
      "   macro avg       0.47      0.40      0.42     11117\n",
      "weighted avg       0.69      0.73      0.70     11117\n",
      "\n",
      "\n",
      "Test Set Metrics:\n",
      "Accuracy: 0.7222\n",
      "Macro F1 Score: 0.4170\n",
      "\n",
      "Top 10 most important features:\n",
      "      feature  importance\n",
      "1     arousal    0.170784\n",
      "10  agreement    0.140568\n",
      "2   dominance    0.108188\n",
      "4     sadness    0.100316\n",
      "3   happiness    0.092476\n",
      "7     disgust    0.080595\n",
      "6        fear    0.078161\n",
      "8    surprise    0.065610\n",
      "5       anger    0.060478\n",
      "0     valence    0.058905\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the stance vectors (assuming you've already created them)\n",
    "df_stance = pd.read_csv('stance_vectors.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_stance.drop('da_tag', axis=1)\n",
    "y = df_stance['da_tag']\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Train classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = clf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "\n",
    "print(f\"Validation Set Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Macro F1 Score: {val_macro_f1:.4f}\")\n",
    "print(\"\\nDetailed Classification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Final evaluation on test set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Macro F1 Score: {test_macro_f1:.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w4705-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
